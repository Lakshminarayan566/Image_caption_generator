{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ Offline Image Caption Generator\n",
    "### Vision-Language Transformer (BLIP) â€” Google Colab\n",
    "\n",
    "**What this does:**  \n",
    "Generates natural language captions for any image using a pretrained BLIP (Bootstrapped Language-Image Pretraining) model from Salesforce. No API key needed â€” model weights download once from HuggingFace and run locally on Colab.\n",
    "\n",
    "---\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Load BLIP model\n",
    "3. Upload your own image OR use a URL\n",
    "4. Generate & display captions\n",
    "5. Batch caption multiple images\n",
    "\n",
    "> âš¡ **Tip:** Go to `Runtime â†’ Change runtime type â†’ T4 GPU` for faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1 â€” Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch torchvision Pillow -q\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 2 â€” Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from google.colab import files\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸  Running on: {device.upper()}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"âš¡ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ Tip: Enable GPU from Runtime â†’ Change runtime type â†’ T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 3 â€” Load BLIP Model\n",
    "\n",
    "> Downloads ~1GB weights on first run. Cached after that â€” subsequent runs are instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Salesforce/blip-image-captioning-large\"\n",
    "\n",
    "print(f\"ğŸ“¥ Loading model: {MODEL_NAME}\")\n",
    "print(\"   (First run downloads ~1GB â€” grab a coffee â˜•)\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "model     = BlipForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"\\nâœ… Model loaded and ready!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Step 4 â€” Caption Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caption-fn-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image: Image.Image,\n",
    "                     prompt: str = None,\n",
    "                     max_length: int = 50,\n",
    "                     num_captions: int = 3,\n",
    "                     num_beams: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Generate captions for a PIL image.\n",
    "\n",
    "    Args:\n",
    "        image        : PIL.Image â€” input image\n",
    "        prompt       : optional text prompt to guide captioning\n",
    "        max_length   : max caption length in tokens\n",
    "        num_captions : number of diverse captions to return\n",
    "        num_beams    : beam search width (higher = better quality, slower)\n",
    "\n",
    "    Returns:\n",
    "        list of caption strings\n",
    "    \"\"\"\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    if prompt:\n",
    "        inputs = processor(image, prompt, return_tensors=\"pt\").to(device)\n",
    "    else:\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Cast to float16 if on GPU\n",
    "    if device == \"cuda\":\n",
    "        inputs = {k: v.half() if v.dtype == torch.float32 else v\n",
    "                  for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=num_captions,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.3,\n",
    "        )\n",
    "\n",
    "    captions = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return [c.strip().capitalize() for c in captions]\n",
    "\n",
    "\n",
    "def show_image_with_captions(image: Image.Image, captions: list, title: str = \"\"):\n",
    "    \"\"\"Display image alongside generated captions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5),\n",
    "                              gridspec_kw={'width_ratios': [1, 1]})\n",
    "\n",
    "    # Left: image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(title or \"Input Image\", fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "    # Right: captions\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_facecolor('#f8f9fa')\n",
    "    caption_text = \"ğŸ¤– Generated Captions\\n\" + \"â”€\" * 38 + \"\\n\\n\"\n",
    "    for i, cap in enumerate(captions, 1):\n",
    "        caption_text += f\"{i}. {cap}\\n\\n\"\n",
    "    axes[1].text(0.05, 0.95, caption_text,\n",
    "                 transform=axes[1].transAxes,\n",
    "                 fontsize=11, verticalalignment='top',\n",
    "                 fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle='round', facecolor='#eef2ff', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Step 5A â€” Upload YOUR Image\n",
    "\n",
    "Run this cell to upload a JPG / PNG image from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image\n",
    "print(\"ğŸ“‚ Select an image to upload...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    image    = Image.open(filename).convert(\"RGB\")\n",
    "\n",
    "    print(f\"\\nâœ… Uploaded: {filename}\")\n",
    "    print(f\"   Size: {image.size[0]}x{image.size[1]} px\")\n",
    "    print(\"\\nâ³ Generating captions...\")\n",
    "\n",
    "    captions = generate_caption(image, num_captions=3, num_beams=5)\n",
    "    show_image_with_captions(image, captions, title=filename)\n",
    "\n",
    "    print(\"\\nğŸ“ Captions:\")\n",
    "    for i, cap in enumerate(captions, 1):\n",
    "        print(f\"   {i}. {cap}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No file uploaded. Try again or use Step 5B below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5b-header",
   "metadata": {},
   "source": [
    "## ğŸŒ Step 5B â€” Use an Image URL (No Upload Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "url-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CHANGE THIS URL to any image you want â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1200px-Cute_dog.jpg\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f\"ğŸŒ Fetching image from URL...\")\n",
    "response = requests.get(IMAGE_URL, timeout=10)\n",
    "image    = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "print(f\"âœ… Image loaded â€” Size: {image.size[0]}x{image.size[1]} px\")\n",
    "print(\"â³ Generating captions...\")\n",
    "\n",
    "captions = generate_caption(image, num_captions=3, num_beams=5)\n",
    "show_image_with_captions(image, captions, title=\"Image from URL\")\n",
    "\n",
    "print(\"\\nğŸ“ Captions:\")\n",
    "for i, cap in enumerate(captions, 1):\n",
    "    print(f\"   {i}. {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Step 6 â€” Prompted Captioning (Guide the Output)\n",
    "\n",
    "Add a text prompt to steer the caption toward a specific aspect of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompted-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CHANGE these to your image + prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1200px-Cute_dog.jpg\"\n",
    "PROMPT    = \"a photo of\"   # try: \"the background shows\", \"the emotion is\", \"there is\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "response = requests.get(IMAGE_URL, timeout=10)\n",
    "image    = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "print(f\"ğŸ’¬ Prompt: '{PROMPT}'\")\n",
    "print(\"â³ Generating prompted captions...\")\n",
    "\n",
    "captions = generate_caption(image, prompt=PROMPT, num_captions=3, num_beams=5)\n",
    "show_image_with_captions(image, captions, title=f\"Prompted: '{PROMPT}'\")\n",
    "\n",
    "print(\"\\nğŸ“ Prompted Captions:\")\n",
    "for i, cap in enumerate(captions, 1):\n",
    "    print(f\"   {i}. {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 7 â€” Batch Caption Multiple Images\n",
    "\n",
    "Upload a folder or list of images â€” captions are generated for all and saved to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# â”€â”€â”€ List of image URLs to batch process â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_URLS = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1200px-Cute_dog.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\",\n",
    "]\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "results = []\n",
    "print(f\"â³ Processing {len(IMAGE_URLS)} images...\\n\")\n",
    "\n",
    "for idx, url in enumerate(IMAGE_URLS, 1):\n",
    "    try:\n",
    "        t0       = time.time()\n",
    "        response = requests.get(url, timeout=10)\n",
    "        image    = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        captions = generate_caption(image, num_captions=1, num_beams=4)\n",
    "        elapsed  = time.time() - t0\n",
    "\n",
    "        results.append({\"image\": url.split(\"/\")[-1], \"caption\": captions[0], \"time_s\": round(elapsed, 2)})\n",
    "        print(f\"  [{idx}/{len(IMAGE_URLS)}] âœ… {url.split('/')[-1]}\")\n",
    "        print(f\"        â†’ {captions[0]}\")\n",
    "        print(f\"        â±  {elapsed:.2f}s\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [{idx}] âŒ Failed: {e}\")\n",
    "        results.append({\"image\": url, \"caption\": \"ERROR\", \"time_s\": 0})\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"captions_output.csv\", index=False)\n",
    "print(\"\\nğŸ“Š Results:\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nğŸ’¾ Saved to captions_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 8 â€” Evaluate with BLEU Score\n",
    "\n",
    "Compare generated captions against reference captions using BLEU metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bleu-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk -q\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# â”€â”€â”€ Add your reference captions here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "reference_captions = [\n",
    "    \"a dog sitting on a grassy field\",\n",
    "    \"a cute brown dog is looking at the camera\",\n",
    "    \"a dog outdoors in nature\"\n",
    "]\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1200px-Cute_dog.jpg\"\n",
    "response  = requests.get(IMAGE_URL, timeout=10)\n",
    "image     = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "generated = generate_caption(image, num_captions=1, num_beams=5)[0].lower()\n",
    "\n",
    "# Tokenize\n",
    "refs      = [ref.split() for ref in reference_captions]\n",
    "hyp       = generated.split()\n",
    "smoother  = SmoothingFunction().method1\n",
    "\n",
    "bleu1 = sentence_bleu(refs, hyp, weights=(1,0,0,0), smoothing_function=smoother)\n",
    "bleu2 = sentence_bleu(refs, hyp, weights=(0.5,0.5,0,0), smoothing_function=smoother)\n",
    "bleu4 = sentence_bleu(refs, hyp, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoother)\n",
    "\n",
    "print(\"ğŸ“Š BLEU Score Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Generated : {generated}\")\n",
    "print(f\"References: {reference_captions}\")\n",
    "print()\n",
    "print(f\"  BLEU-1 : {bleu1:.4f}\")\n",
    "print(f\"  BLEU-2 : {bleu2:.4f}\")\n",
    "print(f\"  BLEU-4 : {bleu4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Summary\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Model** | `Salesforce/blip-image-captioning-large` |\n",
    "| **Architecture** | Vision Transformer (ViT) + BERT decoder |\n",
    "| **Input** | JPG / PNG â€” upload, URL, or batch |\n",
    "| **Output** | 1â€“3 natural language captions per image |\n",
    "| **Evaluation** | BLEU-1, BLEU-2, BLEU-4 |\n",
    "| **Extras** | Prompted captioning, batch CSV export |\n",
    "\n",
    "---\n",
    "**Author:** Maridi Lakshminarayana Â· [github.com/Lakshminarayan566](https://github.com/Lakshminarayan566) Â· maridi.lakshminarayana111@gmail.com"
   ]
  }
 ]
}
